# ================================
# AI Lighthouse API Configuration
# ================================

# Server Configuration
NODE_ENV=development
PORT=3001

# Redis Configuration (Required)
# Used for rate limiting, job storage, and abuse detection
# Local: redis://localhost:6379
# Render: Automatically provided via environment
REDIS_URL=redis://localhost:6379

# CORS Configuration
# Comma-separated list of allowed origins, or * for all
CORS_ORIGIN=http://localhost:3000,https://ai-lighthouse-web.vercel.app

# Logging Configuration
# Levels: error, warn, info, http, verbose, debug, silly
LOG_LEVEL=info

# ================================
# LLM Provider API Keys (Optional)
# ================================

# OpenRouter API Key (default LLM provider)
# Get your free API key at https://openrouter.ai/
# Used as fallback when users don't provide their own key
OPENROUTER_API_KEY=

# Optional: Other LLM providers
# Users can provide their own keys in requests
# OPENAI_API_KEY=
# ANTHROPIC_API_KEY=
# GEMINI_API_KEY=

# ================================
# Rate Limiting Configuration
# ================================

# General rate limit (requests per 15 minutes)
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=10

# LLM-specific rate limit (requests per hour)
LLM_RATE_LIMIT_WINDOW_MS=3600000
LLM_RATE_LIMIT_MAX_REQUESTS=5

# ================================
# Security Configuration
# ================================

# Request timeout in milliseconds
REQUEST_TIMEOUT_MS=120000

# Maximum request body size
MAX_REQUEST_SIZE=1mb

# Abuse detection thresholds
ABUSE_SUSPICION_THRESHOLD=70
ABUSE_BAN_DURATION_SECONDS=3600

# ================================
# Optional: Monitoring & Analytics
# ================================

# Sentry DSN for error tracking (optional)
# SENTRY_DSN=
